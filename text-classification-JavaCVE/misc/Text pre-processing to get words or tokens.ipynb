{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text pre-processing - credit to https://github.com/rohithramesh1991/Text-Preprocessing/blob/master/Text%20Preprocessing_codes.py\n",
    "#Function to clean (text pre-processing)\n",
    "def process_text(text):\n",
    "    '''\n",
    "    Takes in a string of text, then performs the following:\n",
    "    1. Remove all punctuation\n",
    "    2. Remove all stopwords\n",
    "    3. Return the cleaned text as a list of words\n",
    "    4. Remove words\n",
    "    '''\n",
    "    stemmer = WordNetLemmatizer()\n",
    "    nopunc = [char for char in text if char not in string.punctuation]\n",
    "    nopunc = ''.join([i for i in nopunc if not i.isdigit()])\n",
    "    nopunc =  [word.lower() for word in nopunc.split() if word not in stopwords.words('english')]\n",
    "    return[stemmer.lemmatize(word) for word in nopunc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python -m spacy download en_core_web_sm\n",
    "import spacy\n",
    "\n",
    "# Load English tokenizer, tagger, parser and NER\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Process whole documents\n",
    "text = (concatenate_cve (list_of_cve))\n",
    "doc = nlp(text)\n",
    "\n",
    "# Analyze syntax\n",
    "#print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\n",
    "\n",
    "vulnerability_phrases = [chunk.text for chunk in doc.noun_chunks]\n",
    "\n",
    "#print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])\n",
    "\n",
    "# Find named entities, phrases and concepts\n",
    "#for entity in doc.ents:\n",
    "   # print(entity.text, entity.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These require having GCP project, enable billing, authentication, and GCP Language API. Details can be found here: \n",
    "#https://anaconda.org/conda-forge/google-cloud-language\n",
    "import os\n",
    "\n",
    "from google.cloud import language\n",
    "from google.cloud.language import enums\n",
    "from google.cloud.language import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Service account for client authentication \n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"C:/Users/Semiu/Desktop/GCP-AUTH/vulanalyzer-3ff7c0a317fd.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for Text classification using Google API \n",
    "def categorize_vul(corpus):\n",
    "    vul_categories = []\n",
    "    client = language.LanguageServiceClient()\n",
    "    #for i in range(len(corpus)):\n",
    "    document = types.Document(content=corpus,type=enums.Document.Type.PLAIN_TEXT)\n",
    "    response = client.classify_text(document)\n",
    "    \n",
    "    for j in range(len(response.categories)):\n",
    "        #print('Classification of Letter '+ txt_files[i][:-4] )\n",
    "        print((response.categories[j].name))\n",
    "        vul_categories.append(response.categories[j].name)\n",
    "    return vul_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#categorize_vul(list_of_cve)\n",
    "categorize_vul(concatenate_cve (list_of_cve))\n",
    "#concatenate_cve (list_of_cve)\n",
    "#extract_text_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorize_vul(concatenate_cve (list_of_cve))\n",
    "#concatenate_cve (list_of_cve)\n",
    "#extract_text_corpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of clustering models for unspervised learning\n",
    "\n",
    "1. https://stats.stackexchange.com/questions/21807/evaluation-measures-of-goodness-or-validity-of-clustering-without-having-truth\n",
    "\n",
    "2. https://ttic.uchicago.edu/~shubhendu/Papers/clustering_bagging.pdf\n",
    "\n",
    "3. https://arxiv.org/pdf/1202.5695.pdf\n",
    "\n",
    "4. https://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation\n",
    "\n",
    "5. https://arxiv.org/pdf/1905.05667.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
