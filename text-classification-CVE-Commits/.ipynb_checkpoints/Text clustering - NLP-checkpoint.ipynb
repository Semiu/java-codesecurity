{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "visible-workstation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "from future.utils import iteritems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "several-genetics",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import nltk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from ntlk.stem import WordNetLemmatizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE, LocallyLinearEmbedding as LLE\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worldwide-reach",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to extract the text (CVE descriptions)\n",
    "def extract_text_corpus():\n",
    "    \n",
    "    cve_text_corpus = []\n",
    "    \n",
    "    java_cve_text = pd.read_csv(\"C:\\\\Users\\\\Semiu\\\\Documents\\\\java-codesecurity\\\\JavaVulData\\\\JavaVulData.csv\")\n",
    "    \n",
    "    for cve in java_cve_text['vulnerability_description']:\n",
    "        cve_text_corpus.append(cve)\n",
    "        \n",
    "    return cve_text_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "differential-turtle",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_CVEs = extract_text_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advanced-homework",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create stop words from fundamental English stop words\n",
    "stopwords = set(w.rstrip() for w in open('C:\\\\Users\\\\Semiu\\\\Documents\\\\java-codesecurity\\\\text-classification-CVE-Commits\\\\stopwords.txt'))\n",
    "\n",
    "# Add more stopwords specific to this problem\n",
    "stopwords = stopwords.union({\n",
    "    'java', 'se', 'vulnerability', 'sdk', 'remote', 'oracle', 'database', 'server', 'attacker', 'sap', 'netweaver', 'android',\n",
    "'ibm', 'embedded', 'product', 'solution', 'manager', 'xstream', 'allows', 'an', 'unauthenticated', 'component', 'issue',\n",
    "'discovered', 'noise', 'apache', 'xwiki', 'junit', 'discovered', 'unsafe', 'code', 'netscape', 'microsoft', 'machine', 'virtual',\n",
    "'bea', 'navigator', 'serversocket', 'weblogic', 'vm', 'development', 'kit', 'jre', 'unspecified', 'unknown', 'sun', 'runtime', \n",
    "'environment', 'and', '2d', 'web', 'confidentiality', 'integrity', 'unknown', 'vectors', 'products', 'suite', 'in', 'the', 'availability',\n",
    "'update', 'mobile', 'cicso', 'mobility', 'tomcat', 'update', 'in', 'earlier', 'openjdk', 'red', 'hat', 'related', 'postgresql', 'netflix',\n",
    "'issue',})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vital-cuisine",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization function\n",
    "def my_tokenizer(s):\n",
    "    s = s.lower() # downcase\n",
    "    tokens = nltk.tokenize.word_tokenize(s) # split string into words (tokens)\n",
    "    tokens = [t for t in tokens if len(t) > 2] # remove short words, they're probably not useful\n",
    "    tokens = [wordnet_lemmatizer.lemmatize(t) for t in tokens] # put words into base form\n",
    "    tokens = [t for t in tokens if t not in stopwords] # remove stopwords\n",
    "    tokens = [t for t in tokens if not any(c.isdigit() for c in t)] # remove any digits, i.e. \"7.0\"\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respected-sessions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a word-to-index map so that we can create our word-frequency vectors later\n",
    "# let's also save the tokenized versions so we don't have to tokenize again later\n",
    "cve_index_map = {}\n",
    "current_index = 0\n",
    "all_tokens = []\n",
    "all_cves = []\n",
    "index_word_map = []\n",
    "\n",
    "for java_cve in list_of_CVEs:\n",
    "    try:\n",
    "        java_cve = java_cve.encode('ascii', 'ignore') # this will throw exception if bad characters\n",
    "        java_cve = java_cve.decode('utf-8')\n",
    "        all_cves.append(java_cve)\n",
    "        tokens = my_tokenizer(all_cves) #Call the my_tokenizer function\n",
    "        all_tokens.append(tokens)\n",
    "        for token in tokens:\n",
    "            if token not in cve_index_map:\n",
    "                cve_index_map[token] = current_index\n",
    "                current_index += 1\n",
    "                index_word_map.append(token)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swiss-vanilla",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token to vector functon --input matrices - just indicator variables for this example - works better than proportions\n",
    "def tokens_to_vector(tokens):\n",
    "    x = np.zeros(len(cve_index_map))\n",
    "    for t in tokens:\n",
    "        i = word_index_map[t]\n",
    "        x[i] += 1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worthy-consumer",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "N = len(all_tokens)\n",
    "D = len(cve_index_map)\n",
    "X = np.zeros((D, N)) # terms will go along rows, documents along columns\n",
    "i = 0\n",
    "for tokens in all_tokens:\n",
    "    X[:,i] = tokens_to_vector(tokens)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protected-pension",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "def d(u, v):\n",
    "    diff = u - v\n",
    "    return diff.dot(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strange-purple",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "def cost(X, R, M):\n",
    "    cost = 0\n",
    "    for k in range(len(M)):\n",
    "        diff = X - M[k]\n",
    "        sq_distances = (diff * diff).sum(axis=1)\n",
    "        cost += (R[:,k] * sq_distances).sum()\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honest-eagle",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot K-Means Clustering algorithm result\n",
    "def plot_k_means(X, K, index_word_map, max_iter=20, beta=1.0, show_plots=True):\n",
    "    N, D = X.shape\n",
    "    M = np.zeros((K, D))\n",
    "    R = np.zeros((N, K))\n",
    "    exponents = np.empty((N, K))\n",
    "\n",
    "    # initialize M to random\n",
    "    for k in range(K):\n",
    "        M[k] = X[np.random.choice(N)]\n",
    "\n",
    "    costs = np.zeros(max_iter)\n",
    "    for i in range(max_iter):\n",
    "        # step 1: determine assignments / resposibilities\n",
    "        # is this inefficient?\n",
    "        for k in range(K):\n",
    "            for n in range(N):\n",
    "                # R[n,k] = np.exp(-beta*d(M[k], X[n])) / np.sum( np.exp(-beta*d(M[j], X[n])) for j in range(K) )\n",
    "                exponents[n,k] = np.exp(-beta*d(M[k], X[n]))\n",
    "\n",
    "        R = exponents / exponents.sum(axis=1, keepdims=True)\n",
    "\n",
    "        # step 2: recalculate means\n",
    "        for k in range(K):\n",
    "            M[k] = R[:,k].dot(X) / R[:,k].sum()\n",
    "\n",
    "        costs[i] = cost(X, R, M)\n",
    "        if i > 0:\n",
    "            if np.abs(costs[i] - costs[i-1]) < 10e-5:\n",
    "                break\n",
    "\n",
    "    if show_plots:\n",
    "        # plt.plot(costs)\n",
    "        # plt.title(\"Costs\")\n",
    "        # plt.show()\n",
    "\n",
    "        random_colors = np.random.random((K, 3))\n",
    "        colors = R.dot(random_colors)\n",
    "        plt.figure(figsize=(80.0, 80.0))\n",
    "        plt.scatter(X[:,0], X[:,1], s=300, alpha=0.9, c=colors)\n",
    "        annotate1(X, index_word_map)\n",
    "        # plt.show()\n",
    "        plt.savefig(\"test.png\")\n",
    "\n",
    "\n",
    "    # print out the clusters\n",
    "    hard_responsibilities = np.argmax(R, axis=1) # is an N-size array of cluster identities\n",
    "    # let's \"reverse\" the order so it's cluster identity -> word index\n",
    "    cluster2word = {}\n",
    "    for i in range(len(hard_responsibilities)):\n",
    "        word = index_word_map[i]\n",
    "        cluster = hard_responsibilities[i]\n",
    "        \n",
    "    if cluster not in cluster2word:\n",
    "        cluster2word[cluster] = []\n",
    "        cluster2word[cluster].append(word)\n",
    "\n",
    "    # print out the words grouped by cluster\n",
    "    for cluster, wordlist in cluster2word.items():\n",
    "        print(\"cluster\", cluster, \"->\", wordlist)\n",
    "\n",
    "    return M, R\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "matched-makeup",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "def annotate1(X, index_word_map, eps=0.1):\n",
    "    N, D = X.shape\n",
    "    placed = np.empty((N, D))\n",
    "    \n",
    "    for i in range(N):\n",
    "        x, y = X[i]\n",
    "        \n",
    "        # if x, y is too close to something already plotted, move it\n",
    "        close = []\n",
    "        \n",
    "        x, y = X[i]\n",
    "        for retry in range(3):\n",
    "            for j in range(i):\n",
    "                diff = np.array([x, y]) - placed[j]\n",
    "                \n",
    "                # if something is close, append it to the close list\n",
    "                if diff.dot(diff) < eps:\n",
    "                    close.append(placed[j])\n",
    "                    \n",
    "                if close:\n",
    "                    # then the close list is not empty\n",
    "                    x += (np.random.randn() + 0.5) * (1 if np.random.rand() < 0.5 else -1)\n",
    "                    \n",
    "                    y += (np.random.randn() + 0.5) * (1 if np.random.rand() < 0.5 else -1)\n",
    "                    \n",
    "                    close = [] # so we can start again with an empty list\n",
    "                    \n",
    "                else:\n",
    "                    # nothing close, let's break\n",
    "                    break\n",
    "                    \n",
    "        placed[i] = (x, y)\n",
    "                \n",
    "        plt.annotate(\n",
    "            s=index_word_map[i],\n",
    "            xy=(X[i,0], X[i,1]),\n",
    "            xytext=(x, y),\n",
    "            \n",
    "            arrowprops={\n",
    "                'arrowstyle' : '->',\n",
    "                'color' : 'black',\n",
    "            }\n",
    "        )\n",
    "        \n",
    "    print(\"vocab size:\", current_index)\n",
    "    \n",
    "    transformer = TfidfTransformer()\n",
    "    \n",
    "    X = transformer.fit_transform(X).toarray()\n",
    "    \n",
    "    reducer = TSNE()\n",
    "    Z = reducer.fit_transform(X)\n",
    "    \n",
    "    plot_k_means(Z[:,:2], current_index//10, index_word_map, show_plots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excited-indonesia",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
